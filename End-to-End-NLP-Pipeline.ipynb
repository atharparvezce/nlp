{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d37e1c5-be9c-4eff-981f-760bc87d1254",
   "metadata": {},
   "source": [
    "# ✅ **NLP Pipeline**\n",
    "\n",
    "NLP Pipeline is a set of steps followed to build an end-to-end NLP software.  \n",
    "It consists of the following stages:\n",
    "\n",
    "1. **Data Acquisition**  \n",
    "2. **Text Preparation**  \n",
    "   - i. Text Cleanup  \n",
    "   - ii. Basic Preprocessing  \n",
    "   - iii. Advanced Preprocessing  \n",
    "3. **Feature Engineering**  \n",
    "4. **Modeling**  \n",
    "   - i. Model Building  \n",
    "   - ii. Evaluation  \n",
    "5. **Deployment**  \n",
    "   - i. Deployment  \n",
    "   - ii. Monitoring  \n",
    "   - iii. Model Update  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Data Acquisition**\n",
    "It has three stages:\n",
    "\n",
    "### **Stage i: Available Stage**  \n",
    "When data is already present within the organization.  \n",
    "\n",
    "a. **Data on Table**  \n",
    "   - Present in files (CSV, Excel, etc.).  \n",
    "   - Ready for direct use in NLP tasks.  \n",
    "\n",
    "b. **Data in Database**  \n",
    "   - Stored in the company’s internal database.  \n",
    "   - Requires the **Data Engineering team** to extract.  \n",
    "\n",
    "c. **Data is Less**  \n",
    "   - When data is insufficient.  \n",
    "   - Apply **Data Augmentation** techniques:  \n",
    "     - Synonym replacement  \n",
    "     - Bigram flip (swap adjacent words)  \n",
    "     - Back translation  \n",
    "     - Add noise  \n",
    "\n",
    "---\n",
    "\n",
    "### **Stage ii: Data Available Through Others**  \n",
    "When data is not in our system but can be collected externally.  \n",
    "\n",
    "a. **Public Datasets**  \n",
    "   - Freely available (e.g., Kaggle, UCI ML repository).  \n",
    "   - Can be directly downloaded.  \n",
    "\n",
    "b. **Web Scraping**  \n",
    "   - Extract data from websites.  \n",
    "   - Tools: **BeautifulSoup, Scrapy**.  \n",
    "\n",
    "c. **APIs**  \n",
    "   - External APIs provide structured data.  \n",
    "   - Usually in **JSON** format via `requests`.  \n",
    "   - Example: **RapidAPI**.  \n",
    "\n",
    "d. **Files (Unstructured Sources)**  \n",
    "   - PDF → Text Extraction  \n",
    "   - Image → OCR (convert image to text)  \n",
    "   - Audio → Speech-to-Text  \n",
    "\n",
    "---\n",
    "\n",
    "### **Stage iii: Data Not Available at All**  \n",
    "When no dataset exists, we must create our own.  \n",
    "\n",
    "a. **Surveys & Questionnaires**  \n",
    "   - Design forms to collect responses.  \n",
    "   - Good for domain-specific datasets.  \n",
    "\n",
    "b. **Interviews / Feedback Forms**  \n",
    "   - Directly ask users/customers for inputs.  \n",
    "\n",
    "c. **Manual Data Collection**  \n",
    "   - Collect text manually (e.g., product reviews, support tickets).  \n",
    "\n",
    "d. **Crowdsourcing**  \n",
    "   - Distribute tasks to many people online.  \n",
    "   - Platforms: **Amazon Mechanical Turk, Appen**.  \n",
    "   - Useful for labeling or generating text data.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Text Preparation**\n",
    "It has three stages:\n",
    "\n",
    "### **i. Text Cleanup**  \n",
    "Remove unwanted/irrelevant parts of text.  \n",
    "- HTML/Tag Cleaning → remove `<p>, <br>` etc.  \n",
    "- Emoji Removal/Handling → remove or replace emojis.  \n",
    "- Spelling Check → correct typos and spelling mistakes.  \n",
    "\n",
    "---\n",
    "\n",
    "### **ii. Basic Preprocessing**  \n",
    "Prepare text for modeling (two levels: *basic & optional*).  \n",
    "\n",
    "- **Basic**  \n",
    "  - Tokenization → split text into sentences or words.  \n",
    "\n",
    "- **Optional**  \n",
    "  - Stopword Removal → remove common words like *is, the, and*.  \n",
    "  - Stemming → cut words to base/root form (*running → run*).  \n",
    "  - Lemmatization → reduce words to dictionary form (*better → good*).  \n",
    "  - Removing Digits & Punctuation.  \n",
    "  - Lowercasing → unify text format.  \n",
    "  - Language Detection → identify the language of text.  \n",
    "\n",
    "---\n",
    "\n",
    "### **iii. Advanced Preprocessing**  \n",
    "Deeper linguistic analysis.  \n",
    "- POS Tagging → identify parts of speech (noun, verb, adj).  \n",
    "- Parsing → analyze grammatical structure of sentences.  \n",
    "- Coreference Resolution → resolve references (e.g., *“Alice said she is happy” → she = Alice*).  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Feature Engineering**  \n",
    "It has two stages:\n",
    "\n",
    "### **i. Machine Learning (ML) Pipeline**  \n",
    "In ML, we need to **manually create features** because algorithms don’t understand raw text.  \n",
    "\n",
    "**Common Techniques:**  \n",
    "- Bag of Words (BoW) → count frequency of each word.  \n",
    "- TF-IDF (Term Frequency – Inverse Document Frequency) → give weight to important words.  \n",
    "- n-grams → capture word sequences (*e.g., \"good movie\"*).  \n",
    "- Word Embeddings (pre-trained) → Word2Vec, GloVe.  \n",
    "- Custom Features → sentence length, hashtags count, sentiment score, etc.  \n",
    "\n",
    "- In ML pipeline → **manual feature extraction is critical**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **ii. Deep Learning (DL) Pipeline**  \n",
    "In DL, the model itself **learns features automatically** from raw text.  \n",
    "\n",
    "**Common Techniques:**  \n",
    "- Word Embeddings (learned during training) → Word2Vec, GloVe, FastText.  \n",
    "- Contextual Embeddings → **ELMo, BERT, GPT embeddings**.  \n",
    "- End-to-End Learning → raw text → embedding layer → neural network.  \n",
    "\n",
    "- In DL pipeline → **feature engineering is minimal**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Modeling**  \n",
    "It has two stages:\n",
    "\n",
    "### **i. Modeling Approaches**  \n",
    "Ways to build NLP models:  \n",
    "\n",
    "- **Heuristics (Rule-based)**  \n",
    "  - Hand-crafted rules.  \n",
    "  - Example: if sentence contains “?” → it’s a question.  \n",
    "\n",
    "- **Machine Learning (ML)**  \n",
    "  - Traditional algorithms (Logistic Regression, Naive Bayes, SVM).  \n",
    "  - Needs manual feature engineering (BoW, TF-IDF, etc.).  \n",
    "\n",
    "- **Deep Learning (DL)**  \n",
    "  - Neural networks learn features automatically.  \n",
    "  - Examples: RNN, LSTM, Transformers (BERT, GPT).  \n",
    "\n",
    "- **Cloud-based APIs**  \n",
    "  - Ready-made NLP APIs from cloud providers.  \n",
    "  - Examples: Google NLP API, AWS Comprehend, Azure Cognitive Services.  \n",
    "\n",
    "---\n",
    "\n",
    "### **ii. Evaluation**  \n",
    "Once models are built, they need to be evaluated.  \n",
    "\n",
    "- **Intrinsic Evaluation** (direct performance)  \n",
    "  - Accuracy → % correct predictions.  \n",
    "  - Precision, Recall, F1-score → classification quality.  \n",
    "  - BLEU, ROUGE → for text generation/translation.  \n",
    "\n",
    "- **Extrinsic Evaluation** (task-based performance)  \n",
    "  - Real-world performance on end-task.  \n",
    "  - Perplexity → how well a language model predicts text.  \n",
    "  - Task success rate → e.g., chatbot completion rate.  \n",
    "\n",
    "---\n",
    "\n",
    "## **5. Deployment** \n",
    "It has three stages:\n",
    "\n",
    "### **i. Deployment**  \n",
    "- Make the trained model available for real-world use.  \n",
    "- Methods:  \n",
    "  - API (microservices) → expose model as API.  \n",
    "  - Chatbot integration → use in apps/chatbots.  \n",
    "\n",
    "---\n",
    "\n",
    "### **ii. Monitoring**  \n",
    "- Track model performance after deployment.  \n",
    "- Monitor:  \n",
    "  - Accuracy drop.  \n",
    "  - Bias/errors.  \n",
    "  - System performance (latency, response time).  \n",
    "\n",
    "---\n",
    "\n",
    "### **iii. Update**  \n",
    "- Keep the model fresh and effective.  \n",
    "- Methods:  \n",
    "  - Retraining with new data.  \n",
    "  - Fine-tuning on recent examples.  \n",
    "  - Versioning → maintain and roll out new versions.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c485a4c2-394a-4e66-8ecf-8a0e3b9f81cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
